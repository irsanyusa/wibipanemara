<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>When Might AI Outsmart Us? It Depends Who You Ask - JBlogZ</title><script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content="In 1960, Herbert Simon, who went on to win both the Nobel Prize for economics and the Turing Award for computer science, wrote in his book The New Science of Management Decision that machines will be capable, within 20 years, of doing any work that a man can do."><meta name=robots content="index,follow,noarchive"><meta property="og:title" content="When Might AI Outsmart Us? It Depends Who You Ask"><meta property="og:description" content="In 1960, Herbert Simon, who went on to win both the Nobel Prize for economics and the Turing Award for computer science, wrote in his book The New Science of Management Decision that machines will be capable, within 20 years, of doing any work that a man can do."><meta property="og:type" content="article"><meta property="og:url" content="/when-ai-outsmart-humans.html"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-05-31T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-31T00:00:00+00:00"><meta itemprop=name content="When Might AI Outsmart Us? It Depends Who You Ask"><meta itemprop=description content="In 1960, Herbert Simon, who went on to win both the Nobel Prize for economics and the Turing Award for computer science, wrote in his book The New Science of Management Decision that machines will be capable, within 20 years, of doing any work that a man can do."><meta itemprop=datePublished content="2024-05-31T00:00:00+00:00"><meta itemprop=dateModified content="2024-05-31T00:00:00+00:00"><meta itemprop=wordCount content="2135"><meta itemprop=keywords content><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/mainroad/css/style.css><link rel="shortcut icon" href=./favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=./index.html title=JBlogZ rel=home><div class="logo__item logo__text"><div class=logo__title>JBlogZ</div></div></a></div><div class=divider></div></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>When Might AI Outsmart Us? It Depends Who You Ask</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2024-05-31T00:00:00Z>May 31, 2024</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=./categories/blog/ rel=category>blog</a></span></div></div></header><div class="content post__content clearfix"><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><span class="leading-7 float-left border-t-2 border-l-2 border-solid border-time-red text-5xl py-2 pr-0.5 pl-[0.3125rem] my-0.5 mr-2.5 font-zilla-slab">I</span>n 1960, Herbert Simon, who went on to win both the Nobel Prize for economics and the Turing Award for computer science, <a href=#>wrote</a> in his book The New Science of Management Decision that “machines will be capable, within 20 years, of doing any work that a man can do.”&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">History is filled with exuberant technological predictions that have failed to materialize. Within the field of artificial intelligence, the brashest predictions have concerned the arrival of systems that can perform any task a human can, often referred to as artificial general intelligence, or AGI.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">So when Shane Legg, Google DeepMind’s co-founder and chief AGI scientist, <a href=#>estimates</a> that there’s a 50% chance that AGI will be developed by 2028, it might be tempting to write him off as another AI pioneer who hasn’t learnt the lessons of history.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Still, AI is certainly progressing rapidly. GPT-3.5, the language model that powers OpenAI’s ChatGPT was developed in 2022, and scored 213 out of 400 on the Uniform Bar Exam, the standardized test that prospective lawyers must pass, putting it in the bottom 10% of human test-takers. GPT-4, developed just months later, scored 298, putting it in the top 10%. Many experts expect this progress to continue.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><strong>Read More:</strong> <a href=#>4 Charts That Show Why AI Progress Is Unlikely to Slow Down</a></p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Legg’s views are common among the leadership of the companies currently building the most powerful AI systems. In August, <a href=#>Dario Amodei</a>, co-founder and CEO of Anthropic, <a href=#>said</a> he expects a “human-level” AI could be developed in two to three years. Sam Altman, CEO of OpenAI, <a href=#>believes</a> AGI could be reached sometime in the next four or five years.&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">But in a recent <a href=#>survey</a> the majority of 1,712 AI experts who responded to the question of when they thought AI would be able to accomplish every task better and more cheaply than human workers were less bullish. A separate <a href=#>survey</a> of elite forecasters with exceptional track records shows they are less bullish still.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The stakes for divining who is correct are high. Legg, like many other AI pioneers, has <a href=#>warned</a> that powerful future AI systems could cause human extinction. And even for those less concerned by <a href=#>Terminator</a> scenarios, some warn that an AI system that could replace humans at any task might <a href=#>replace human labor entirely</a>.</p><h2 class="text-2xl font-bold tracking-0.5px font-zilla-slab self-baseline">The scaling hypothesis</h2><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Many of those working at the companies building the biggest and most powerful AI models believe that the arrival of AGI is imminent. They subscribe to a theory known as the scaling hypothesis: the idea that even if a few incremental technical advances are required along the way, continuing to train AI models using ever greater amounts of computational power and data will inevitably lead to AGI.&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">There is some evidence to back this theory up. Researchers have observed very neat and predictable relationships between how much computational power, also known as “compute,” is used to train an AI model and how well it performs a given task. In the case of large language models (LLM)—the AI systems that power chatbots like ChatGPT—scaling laws predict how well a model can predict a missing word in a sentence. OpenAI CEO Sam Altman recently <a href=#>told</a> TIME that he realized in 2019 that AGI might be coming much sooner than most people think, after OpenAI researchers <a href=#>discovered</a> the scaling laws.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px"><strong>Read More:</strong> <a href=#>2023 CEO of the Year: Sam Altman</a></p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Even before the scaling laws were observed, researchers have long understood that training an AI system using more compute makes it more capable. The amount of compute being used to train AI models has <a href=#>increased</a> relatively predictably for the last 70 years as costs have fallen.&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Early predictions based on the expected growth in compute were used by experts to anticipate when AI might match (and then possibly surpass) humans. In 1997, computer scientist Hans Moravec <a href=#>argued</a> that cheaply available hardware will match the human brain in terms of computing power in the 2020s. An Nvidia A100 semiconductor chip, widely used for AI training, costs around $10,000 and can perform roughly 20 trillion FLOPS, and chips developed later this decade will have higher performance still. However, estimates for the amount of compute used by the human brain vary widely from around <a href=#>one trillion</a> floating point operations per second (FLOPS) to more than <a href=#>one quintillion</a> FLOPS, making it hard to evaluate Moravec’s prediction. Additionally, training modern AI systems requires a great deal more compute than running them, a fact that Moravec’s prediction did not account for.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">More recently, researchers at nonprofit Epoch have made a more sophisticated compute-based <a href=#>model</a>. Instead of estimating when AI models will be trained with amounts of compute similar to the human brain, the Epoch approach makes direct use of scaling laws and makes a simplifying assumption: If an AI model trained with a given amount of compute can faithfully reproduce a given portion of text—based on whether the scaling laws predict such a model can repeatedly predict the next word almost flawlessly—then it can do the work of producing that text. For example, an AI system that can perfectly reproduce a book can substitute for authors, and an AI system that can reproduce scientific papers without fault can substitute for scientists.&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Some would argue that just because AI systems can produce human-like outputs, that doesn’t necessarily mean they will think like a human. After all, Russell Crowe plays Nobel Prize-winning mathematician John Nash in the 2001 film, A Beautiful Mind, but nobody would claim that the better his acting performance, the more impressive his mathematical skills must be. Researchers at Epoch <a href=#>argue</a> that this analogy rests on a flawed understanding of how language models work. As they scale up, LLMs acquire the ability to reason like humans, rather than just superficially emulating human behavior. However, some researchers <a href=#>argue</a> it's unclear whether current AI models are in fact reasoning.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Epoch’s approach is one way to quantitatively model the scaling hypothesis, says Tamay Besiroglu, Epoch’s associate director, who notes that researchers at Epoch tend to think AI will progress less rapidly than the model suggests. The model estimates a 10% chance of transformative AI—<a href=#>defined</a> as “AI that if deployed widely, would precipitate a change comparable to the industrial revolution”—being developed by 2025, and a 50% chance of it being developed by 2033. The difference between the model’s forecast and those of people like Legg is probably largely down to transformative AI being harder to achieve than AGI, says Besiroglu.</p><h2 class="text-2xl font-bold tracking-0.5px font-zilla-slab self-baseline">Asking the experts</h2><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Although many in leadership positions at the most prominent AI companies believe that the current path of AI progress will soon produce AGI, they’re outliers. In an effort to more systematically assess what the experts believe about the future of artificial intelligence, AI Impacts, an AI safety project at the nonprofit Machine Intelligence Research Institute, <a href=#>surveyed</a> 2,778 experts in fall 2023, all of whom had published peer-reviewed research in prestigious AI journals and conferences in the last year.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Among other things, the experts were asked when they thought “high-level machine intelligence,” defined as machines that could “accomplish every task better and more cheaply than human workers” without help, would be feasible. Although the individual predictions varied greatly, the average of the predictions suggests a 50% chance that this would happen by 2047, and a 10% chance by 2027.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Like many people, the experts seemed to have been surprised by the rapid AI progress of the last year and have updated their forecasts accordingly—when AI Impacts ran the same survey in 2022, researchers estimated a 50% chance of high-level machine intelligence arriving by 2060, and a 10% chance by 2029.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The researchers were also asked when they thought various individual tasks could be carried out by machines. They estimated a 50% chance that AI could compose a Top 40 hit by 2028 and write a book that would make the New York Times bestseller list by 2029.</p><h2 class="text-2xl font-bold tracking-0.5px font-zilla-slab self-baseline">The superforecasters are skeptical</h2><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Nonetheless, there is plenty of evidence to suggest that experts don’t make good forecasters. Between 1984 and 2003, social scientist Philip Tetlock collected 82,361 forecasts from 284 experts, asking them questions such as: Will Soviet leader Mikhail Gorbachev be ousted in a coup? Will Canada survive as a political union? Tetlock found that the experts’ predictions were often no better than chance, and that the more famous an expert was, the less accurate their predictions tended to be.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Next, Tetlock and his collaborators set out to determine whether anyone could make accurate predictions. In a forecasting competition <a href=#>launched</a> by the U.S. Intelligence Advanced Research Projects Activity in 2010, Tetlock’s team, the Good Judgement Project (GJP), dominated the others, producing forecasts that were reportedly 30% more accurate than intelligence analysts who had access to classified information. As part of the competition, the GJP identified “superforecasters”—individuals who consistently made above-average accuracy forecasts. However, although superforecasters have been shown to be reasonably accurate for predictions with a time horizon of two years or less, it's unclear whether they’re also similarly accurate for longer-term questions such as when AGI might be developed, says Ezra Karger, an economist at the Federal Reserve Bank of Chicago and research director at Tetlock’s Forecasting Research Institute.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">When do the superforecasters think AGI will arrive? As part of a <a href=#>forecasting tournament</a> run between June and October 2022 by the Forecasting Research Institute, 31 superforecasters were asked when they thought Nick Bostrom—the controversial philosopher and author of the seminal AI existential risk treatise Superintelligence—would affirm the existence of AGI. The median superforecaster thought there was a 1% chance that this would happen by 2030, a 21% chance by 2050, and a 75% chance by 2100.</p><h2 class="text-2xl font-bold tracking-0.5px font-zilla-slab self-baseline">Who’s right?</h2><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">All three approaches to predicting when AGI might be developed—Epoch’s model of the scaling hypothesis, and the expert and superforecaster surveys—have one thing in common: there’s a lot of uncertainty. In particular, the experts are spread widely, with 10% thinking it's as likely as not that AGI is developed by 2030, and 18% thinking AGI won’t be reached until after 2100.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Still, on average, the different approaches give different answers. Epoch’s model estimates a 50% chance that transformative AI arrives by 2033, the median expert estimates a 50% probability of AGI before 2048, and the superforecasters are much further out at 2070.</p><img style=margin:auto;display:block;text-align:center;max-width:100%;height:auto src=https://cdn.statically.io/img/api.time.com/wp-content/uploads/2024/01/forecastingforagicharts.png><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">There are many points of disagreement that feed into debates over when AGI might be developed, says Katja Grace, who organized the expert survey as lead researcher at AI Impacts. First, will the current methods for building AI systems, bolstered by more compute and fed more data, with a few algorithmic tweaks, be sufficient? The answer to this question in part depends on how impressive you think recently developed AI systems are. Is GPT-4, in the words of researchers at Microsoft, the <a href=#>sparks of AGI</a>? Or is this, in the <a href=#>words</a> of philosopher Hubert Dreyfus, “like claiming that the first monkey that climbed a tree was making progress towards landing on the moon?”</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Second, even if current methods are enough to achieve the goal of developing AGI, it's unclear how far away the finish line is, says Grace. It’s also possible that something could obstruct progress on the way, for example a shortfall of training data.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Finally, looming in the background of these more technical debates are people’s more fundamental beliefs about how much and how quickly the world is likely to change, Grace says. Those working in AI are often steeped in technology and open to the idea that their creations could alter the world dramatically, whereas most people dismiss this as unrealistic.</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">The stakes of resolving this disagreement are high. In addition to asking experts how quickly they thought AI would reach certain milestones, AI Impacts asked them about the technology’s societal implications. Of the 1,345 respondents who answered questions about AI’s impact on society, 89% said they are substantially or extremely concerned about AI-generated deepfakes and 73% were similarly concerned that AI could empower dangerous groups, for example by enabling them to engineer viruses. The median respondent thought it was 5% likely that AGI leads to “extremely bad,” outcomes, such as human extinction.&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Given these concerns, and the fact that 10% of the experts surveyed believe that AI might be able to do any task a human can by 2030, Grace argues that policymakers and companies should prepare now.&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">Preparations could include investment in safety research, mandatory safety testing, and coordination between companies and countries developing powerful AI systems, says Grace. Many of these measures were also recommended in a <a href=#>paper</a> published by AI experts last year.&nbsp;</p><p class="self-baseline px-0 font-pt-serif text-17px leading-7 tracking-0.5px">“If governments act now, with determination, there is a chance that we will learn how to make AI systems safe before we learn how to make them so powerful that they become uncontrollable,” Stuart Russell, professor of computer science at the University of California, Berkeley, and one of the paper’s authors, told TIME in October.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmismaKyb6%2FOpmZvbWVrfneEjrCfnqZdlrZuu9StqqaZoql6qcHMmqWsZw%3D%3D</p></div></article></main><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=./1607938-bbnaijas-doyin-slams-men-split-bills-5050-wives-sparks-debate-s-a-roommate.html rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>BBNaijas Doyin Slams Men Who Split Bills 5050 With Their Wives, Sparks Debate: Hes a Roo</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=./un-nouveau-decret-modifie-les-regles-du-jeu.html rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>Un nouveau dcret modifie les rgles du jeu</p></a></div></nav></div><aside class=sidebar><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=./joe-biden-immigration-private-prisons.html>Biden Use of Private Migrant Centers Grows Amid Border Surge</a></li><li class=widget__item><a class=widget__link href=./how-often-should-you-replace-your-shoes.html>How Often Should You Replace Your Shoes</a></li><li class=widget__item><a class=widget__link href=./1550102-james-bond-actors-order-how-played-iconic-spy.html>James Bond actors in order: how many played the iconic spy?</a></li><li class=widget__item><a class=widget__link href=./0-28804-1898067-1938106-1938103-00-html.html>Jeff Han - TIME 100 Roundtables</a></li><li class=widget__item><a class=widget__link href=./la-cnsa-adopte-sa-convention-socle-avec-les-departements.html>La CNSA adopte sa convention socle avec les dpartements</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=./categories/blog/>blog</a></li></ul></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2024 JBlogZ.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=https://assets.cdnweb.info/hugo/mainroad/js/menu.js></script>
<script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/banner.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>